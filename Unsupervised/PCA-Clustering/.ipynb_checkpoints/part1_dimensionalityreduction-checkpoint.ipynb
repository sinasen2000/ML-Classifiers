{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Unsupervised Learning\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma and Gosia Migut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on unsupervised learning with k-means clustering and PCA. \n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of unsupervised learning.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. There is quite a bit of theory and explanation in these notebooks. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/town-square\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning without examples\n",
    "\n",
    "When we want to learn from data without knowing what the labels are, we apply unsupervised learning. These are techniques to make sense of our data from the data itself. An example of a task for which unsupervised learning is useful and that you will implement is dimensionality reduction: trying to find the 'essential' features or combinations of features to describe objects. In Part 2 of this assignment, you will apply your dimensionality reduction algorithm and practice with k-Means clustering.\n",
    "\n",
    "### Structure\n",
    "\n",
    "This assignment consists of two parts:\n",
    "- In [Part 1](part1_dimensionalityreduction.ipynb), you will get familiar with dimensionality reduction using PCA.\n",
    "- In [Part 2](part2_clustering.ipynb), you will apply your PCA algorithm and practice with k-Means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two exercises you will create an algorithm to perform dimension reduction on\n",
    "datasets. To test the algorithm you will reduce the dimensions of a 2D Gaussian dataset as well as\n",
    "a set of images of faces. The method for calculating the eigenvectors will be the Power Iteration\n",
    "method, which will be implemented first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions, scroll down to start the exercise\n",
    "\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_data(data, eigen_vectors = None):\n",
    "    \"\"\"\n",
    "    This function plots the data of the given `eigen_vectors` with a scatterplot of the matrix data. \n",
    "    If no eigen vectors are available, it just plots the data\n",
    "    :param data: the data\n",
    "    :param eigen_vectors: the eigenvectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot the features as a scatterplot\n",
    "    x = [[el[0]] for el in data]\n",
    "    y = [[el[1]] for el in data]\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    if eigen_vectors:\n",
    "        # Plot the two PCA lines\n",
    "        for vector in eigen_vectors:\n",
    "            line = _set_line(vector)\n",
    "            plt.plot(line[0], line[1], 'red')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _set_line(vector):\n",
    "    # Fixed number for the line size of this plot\n",
    "    line_size = 6\n",
    "\n",
    "    # Set the coordinates for the PCA lines\n",
    "    axis = np.zeros((2, 2))\n",
    "    axis[0][0] = vector[0] * line_size\n",
    "    axis[1][0] = vector[1] * line_size\n",
    "    axis[0][1] = vector[0] * -line_size\n",
    "    axis[1][1] = vector[1] * -line_size\n",
    "    return axis\n",
    "\n",
    "\n",
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    This function loads a given matrix data file into a numpy matrix.\n",
    "    :param file_name: name of the file to be read\n",
    "    :return: the data as a numpy array\n",
    "    \"\"\"\n",
    "    lines = [line.rstrip('\\n') for line in open(file_name)]\n",
    "\n",
    "    result = np.zeros((len(lines), len(lines[0].split(\" \"))))\n",
    "\n",
    "    for (i, line) in enumerate(lines):\n",
    "        line = line.split(\" \")\n",
    "        for (j, number) in enumerate(line):\n",
    "            result[i][j] = float(number)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_image(fv):\n",
    "    \"\"\"\n",
    "    This function creates a grey image based on the given feature vector `fv`.\n",
    "    :param fv: the feature vector\n",
    "    :param title: the title of the image\n",
    "    \"\"\"\n",
    "    width = height = int(math.sqrt(len(fv)))\n",
    "\n",
    "    # Filter label and threshold from data\n",
    "    img = Image.new('L', (width, height), \"black\")\n",
    "    pixels = img.load()\n",
    "    min_v = min(fv)\n",
    "    max_v = max(fv)\n",
    "\n",
    "    # Iterate over each pixel and set p value\n",
    "    j = 0\n",
    "    for (idx, p) in enumerate(fv):\n",
    "        i = idx % width\n",
    "        pixel = int(((p - min_v) / (max_v - min_v) * 255))\n",
    "        pixels[i, j] = pixel\n",
    "        if i == (width - 1):\n",
    "            j += 1\n",
    "\n",
    "    # Resize image to make it better visible\n",
    "    img = img.resize((256, 256), Image.ANTIALIAS)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Iteration\n",
    "\n",
    "Consider a square $(D × D)$ matrix $M$, and a $D × 1$ column vector **v** which is an eigenvector of $M$, with eigenvalue $λ ∈ \\mathbb{R}$.\n",
    "\n",
    "$\\q{1.1}$ What equation defines the relation between $M$, $\\mathbf{v}$, and $λ$?  Write down this equation.\n",
    "\n",
    "The Power Iteration method is a relatively simple method for calculating eigenvectors for a square $(D × D)$ matrix $M$. The process works as follows:\n",
    "1. Construct a vector $\\mathbf{v}_0$ of ones of length $D × 1$.\n",
    "2. Until convergence, compute:  \n",
    "<center>$\\mathbf{v}_{k+1} = \\dfrac{M\\mathbf{v}_{k}}{||M\\mathbf{v}_{k}||}$  </center>\n",
    "Where $||M\\mathbf{v}_{k}||$ is the (L2) norm of $M\\mathbf{v}_{k}$.\n",
    "3. Output vector $\\mathbf{}v$ as the principal eigenvector of $M$.\n",
    "4. Compute $M^∗$ as:  \n",
    "<center>$λ = \\mathbf{v}^\\intercal M\\mathbf{v}$  </center>\n",
    "<center>$M^∗ = M − λ × \\mathbf{vv}^\\intercal$</center>\n",
    "5. If more eigenvectors are required, go to step 1 with $M^∗$ as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.1}$ Finish the `power_iteration()` function. This method will calculate `n_vectors` eigenvectors from the square matrix `matrix`. Use the steps described above to do so. The stopping criterion in the second step should be when the L2 norm of the difference between $v_{k+1}$ and $v_k$ of two consecutive iterations is smaller than the convergence parameter `e`.\n",
    "\n",
    "__Hint__ The L2 norm can be computed with `la.norm(x)` which is the same as computing $\\sqrt{x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "def power_iteration(matrix, n_vectors, e):\n",
    "    \"\"\"\n",
    "    This function returns a list with `n_vectors` amount of eigenvectors (numpy vectors) based on the given square \n",
    "    `matrix` and the convergence parameter `e`.\n",
    "    :param matrix: the square matrix\n",
    "    :param n_vectors: the number of eigenvectors\n",
    "    :param e: the convergence parameter\n",
    "    :return: the list of eigenvectors found\n",
    "    \"\"\"\n",
    "    assert (matrix.shape[0] == matrix.shape[1] & matrix.shape[1] >= n_vectors)\n",
    "\n",
    "    eigen_vectors = list()\n",
    "    # -------------------------------Student ---------------------------------------\n",
    "\n",
    "   \n",
    "    # -------------------------------end Student ---------------------------------------        \n",
    "    return eigen_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.2}$ How many eigenvectors could there possibly be in a $D × D$ matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.2}$ Create a matrix and let it read the data from `data/matrix.txt`. Use the `power_iteration()` method to calculate two eigenvectors from this matrix (and set `e` to something like 10E-5). Verify that the resulting eigenvectors are roughly equal to:  \n",
    "\n",
    "<center>$v_1 = \\begin{bmatrix}0.4472 \\\\ 0.8944\\end{bmatrix}$</center>  \n",
    "<center>$v_2 = \\begin{bmatrix}0.8944 \\\\ -0.4472\\end{bmatrix}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call power_iteration function for this exercise\n",
    "\n",
    "data = read_data(\"data/matrix.txt\")\n",
    "\n",
    "# -------------------------------Student ---------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------end Student ---------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.3}$ Are the eigenvalues of these eigenvectors increasing or decreasing as you compute more eigenvectors? What do these eigenvalues say about the eigenvectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use Principal Component Analysis to find the principal components in some dataset.\n",
    "Principal components can be seen as vectors along which most variance is found in the data. We will now create a matrix that reads data from `data/gaussian.txt`. The size of this matrix is N × 2, where N is the number of points in the dataset. We then use `plot_data()` to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"data/gaussian.txt\")\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.1}$\n",
    "By just looking at the plotted data, can you predict the direction of the first principal component? What about the second principal component?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2.1}$\n",
    "The principal components of a dataset can also be seen as the eigenvectors of the covariance\n",
    "matrix. Compute the covariance matrix of the Gaussian dataset as follows:  \n",
    "  \n",
    "<center>$cov(X) = \\dfrac{1}{N}(X - \\bar{x})^\\intercal(X - \\bar{x})$  </center>\n",
    "\n",
    "where $\\bar{x}$ is the mean row of $N × D$ dimensional data matrix $X$. Next, compute the eigenvectors of this covariance matrix and plot the vectors accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(data):\n",
    "    \"\"\"\n",
    "    This function computes the computes the covariance matrix of a given `data`.\n",
    "    :param data: the starting data\n",
    "    :return: the covariance matrix \n",
    "    \"\"\"\n",
    "    # -------------------------------Student ---------------------------------------\n",
    "   \n",
    "    # -------------------------------end Student ---------------------------------------\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"data/gaussian.txt\")\n",
    "matrix = covariance(data)\n",
    "\n",
    "# You can check your implementation with the numpy built-in cov function.\n",
    "# Your value might differ slightly as the numpy built-in cov function is a bit more precise than our function :).\n",
    "matrix_np = np.cov(np.transpose(data))\n",
    "err_msg=\"Your covariance matrix is allowed to be slight less precise but it should not differ more ~+-0.025\"\n",
    "np.testing.assert_allclose(matrix_np, matrix, atol=0.025, err_msg=err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use power iteration method to compute eigen vectors using your covariance matrix and plot the result.\n",
    "# The plot should contain both the dataset (just like the previous one) and the eigen vectors.\n",
    "# -------------------------------Student ---------------------------------------\n",
    "\n",
    "# -------------------------------end Student ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.2}$ Which of these eigenvectors captures the most variance? Was this the first principal component or the second?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2.2}$ Create a matrix object and let it read in `data/faces.txt`. This matrix is of size $N × D$, where $N$ is the number of images in the dataset and $D$ is the number of pixels per image (in this case 32 × 32, so D = 1024). Similar to before, compute the covariance matrix. Use this covariance matrix to compute the first 10 principal components and visualize them using numpy. Visualize the principal components using the given `create_image()` function.  \n",
    "  \n",
    "__Note__ `create_image()` only accepts rows of the dataset (`lists`), use the `transpose` function of `numpy` to convert columns to rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display # to display images, usage: display(image)\n",
    "data = read_data(\"data/faces.txt\")\n",
    "mean = np.mean(data, axis=0)\n",
    "\n",
    "# Image of the mean\n",
    "print(\"mean image\")\n",
    "mean_image = create_image(mean)\n",
    "display(mean_image)\n",
    "# Now plot the image of each of the eigenvectors of the given dataset\n",
    "# -------------------------------Student ---------------------------------------\n",
    "\n",
    "# -------------------------------end Student ---------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.3}$ What do these principal components mean, in terms of faces?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
