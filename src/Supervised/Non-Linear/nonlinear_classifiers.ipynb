{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Non-Linear Classifiers\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma, Odette Scharenborg and the TA's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on decision trees.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of non-linear classifiers, in particular decision trees.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/town-square\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this assignment, you will build an implementation of a univariate decision tree algorithm that can classify using both discrete and numeric variables.\n",
    "\n",
    "Decision trees are non-linear classifiers. In other words: we can separate data with a decision boundary that does not resemble a single line. Why would this be useful? \n",
    "\n",
    "Think of the x-or problem, given by the following points:\n",
    "\n",
    "- $X = (0, 1); (1, 0);$ \n",
    "\n",
    "- $O = (0, 0); (1, 1);$\n",
    "\n",
    "```\n",
    "x   o\n",
    "\n",
    "o   x\n",
    "```\n",
    "$\\ex{0.1}$ Try to draw a line that can separate the x's from the o's.\n",
    "\n",
    "Quite impossible, right? Now let's try this with a decision tree: we set up a tree where each node represents a decision and the children of a node represent the choices for this decision. An example of such a tree for the above problem could be:\n",
    "\n",
    "```\n",
    "          x < 0.5?\n",
    "           /   \\\n",
    "         yes   no\n",
    "         /       \\\n",
    "    y < 0.5?    y < 0.5?\n",
    "      /  \\       /  \\\n",
    "    yes  no    yes   no\n",
    "    /     \\     /     \\\n",
    "   o      x    x       o\n",
    "```\n",
    "\n",
    "$\\ex{0.2}$ Follow the decision tree for each point in the x-or problem and verify that the points are classified correctly.\n",
    "\n",
    "You will implement the code to evaluate such a decision tree.\n",
    "\n",
    "## 1. The dataset: heart disease \n",
    "\n",
    "We will use decision trees to predict whether a patient has a heart disease using a dataset containing symptoms, prescriptions, and diagnoses from four different hospitals. The dataset can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/).\n",
    "\n",
    "A description of the dataset is provided in the file named [heart-disease.names.txt](data/heart-disease.names.txt). Scan through the file and take a good look at sections 4 and 7. This section describes the different variables which we will use.\n",
    "\n",
    "The dataset contains both discrete and continuous variables:\n",
    "- An example of a discrete variable is attribute #9, *chest pain type*, with four different possible labels for the chest pain type.\n",
    "- An example of a continuous variable is attribute #12, *serum cholestoral in mg/dl*, with the concentration of cholesterol in mg/dl.\n",
    "\n",
    "Decision trees are particularly good at handling both discrete and continuous variables, so they could be a good classifier for our dataset.\n",
    "\n",
    "### Understanding the dataset\n",
    "\n",
    "We have created a cleaned-up version of the dataset where patient records from all four hospitals are aggregated together. You can load it in using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 14)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('data/heart_disease.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us 299 patient records, most of which are from the Cleveland hospital. For each patient we have 14 features, but we don't know which ones are discrete and which ones are continuous yet.\n",
    "\n",
    "Instead of studying the description of each feature, we will try to find out by counting the number of unique values for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 41,   2,   4,  50, 153,   2,   3,  92,   2,  40,   3,   4,   3,\n",
       "         5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data = np.sort(data, axis=0)\n",
    "frequencies = (sorted_data[1:,:] != sorted_data[:-1,:]).sum(axis=0) + 1\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ Which features are likely to be discrete and which numeric?\n",
    "\n",
    "We also see that the last column, the actual diagnosis, has 5 possible values. These are our labels.\n",
    "\n",
    "Let's visualise how these labels are distributed using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([160.,  56.,  35.,  35.,  13.]),\n",
       " array([-0.5,  0.5,  1.5,  2.5,  3.5,  4.5]),\n",
       " <a list of 5 Patch objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline \n",
    "\n",
    "# Ignore certain warnings.\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "plt.hist(data[:, 13], np.arange(0, 4 + 1.5) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.2}$ What stands out in this histogram? What does this mean in terms of diagnosis?\n",
    "\n",
    "We will create a univariate decision tree, which means that it will decide if an entry belongs to one specific diagnosis or not (binary decision). Because of the distribution of diagnoses, it seems logical to decide for each entry whether it belongs to class $0$ or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "In order to train and validate the decision trees, we split the dataset into a training and validation set and separate each of these into three arrays:\n",
    "\n",
    "1. `x_discrete`, a 2d-array of integers containing the discrete variables for each patient.\n",
    "2. `x_numeric`, a 2d-array of floats containing the numeric variables for each patient.\n",
    "3. `y`, a 1d-array of booleans indicating for each patient whether the diagnoses was class 0 or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the array into features and labels\n",
    "x = data[:, :13]\n",
    "y = data[:, 13]\n",
    "\n",
    "# Transform classes to booleans\n",
    "# y = (y == 0), Numpy will repeat this equality check for each entry in the array\n",
    "# and return an array of booleans.\n",
    "y = y == np.zeros(len(y))\n",
    "\n",
    "def split_dataset(x, y, random_state):\n",
    "    # Split data into train and validation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # For this assignment, we state the random_state variable.\n",
    "    # This variable will be used as the seed for the random number generation so that the split is deterministic.\n",
    "    # Therefore, all exercises will give the same results every run.\n",
    "    x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=0.3, random_state=random_state) \n",
    "\n",
    "    # Separate features into discrete and numeric arrays\n",
    "    x_train_discrete = x_train[:, np.where(frequencies < 5)[0]].astype(int)\n",
    "    x_train_numeric = x_train[:, np.where(frequencies > 5)[0]]\n",
    "    x_validation_discrete = x_validation[:, np.where(frequencies < 5)[0]].astype(int)\n",
    "    x_validation_numeric = x_validation[:, np.where(frequencies > 5)[0]]\n",
    "    \n",
    "    return x_train_discrete, x_train_numeric, x_validation_discrete, x_validation_numeric, y_train, y_validation\n",
    "\n",
    "x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.1}$ Print the six arrays and check that the shapes are correct. Verify that `y_train` has the same number of rows as `x_train_disc` and `x_train_num`, verify that `y_validation` has the same number of rows as `x_validation_disc` and `x_validation_num`, and verify that there are 8 discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 0 ... 1 1 3]\n",
      " [1 4 0 ... 3 0 7]\n",
      " [0 2 0 ... 1 0 3]\n",
      " ...\n",
      " [1 3 0 ... 2 1 7]\n",
      " [1 1 0 ... 1 2 3]\n",
      " [0 3 1 ... 1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entropy and Information Gain\n",
    "\n",
    "A decision tree splits the dataset based on the value of a certain feature. To find the best features and values to split on, we need some way to measure the quality of a split. We will use entropy and information gain for this purpose.\n",
    "\n",
    "<img src=\"images/entropy-split.png\" alt=\"Entropy Split\" style=\"height: 350px;\"/>\n",
    "\n",
    "*Image retrieved from: https://bricaud.github.io/personal-blog/entropy-in-decision-trees*\n",
    "\n",
    "\n",
    "For decision trees we use the information theoretic entropy, also known as Shannon entropy. This tells us something about the amount of information contained in a certain disctribution.\n",
    "\n",
    "The best split is the split that leads to the highest frequency of '1's in one set and the highest frequency of '0's in the other set. Entropy can thus also be regarded as a measure of purity, and we aim for the purest classes. To calculate the entropy of a split, first the entropy of the individual classes are calculated, after which they are combined. For instance, an entropy close to 1.0 indicates that a certain split doesn't tell us much about the class that corresponds to the data because the data in the resulting split is divided evenly.\n",
    "\n",
    "<img src=\"images/entropy.png\" alt=\"Entropy\" style=\"height: 350px;\"/>\n",
    "\n",
    "*In this graph, we plot the relation between entropy and the proportion of data points belonging to one class (in this case '+'). The entropy is maximal when the data is split 50/50. The entropy decreases as the data set becomes 'purer'. Our goal is to decrease the entropy by making proper splits.*\n",
    "\n",
    "*Image retrieved from: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8*\n",
    "\n",
    "\n",
    "The Shannon entropy for any number of classes is given as:\n",
    "\n",
    "(2.1.a) $$\\phi(p) = −\\sum_i p_i\\ log_2(p_i)$$\n",
    "\n",
    "As mentioned, we will only decide whether an entry belongs to class $0$ or not: True or False. Thus, in or our case, we can write the Shannon entropy as follows:\n",
    "\n",
    "(2.1.b) $$\\phi(p) = −p\\ log_2(p) − (1 − p)\\ log_2(1 − p)$$\n",
    "\n",
    "where $p$, the probability that an item has label 0, is equivalent to the ratio between the number of items with label $0$ (True) and the number of items with another label (False).\n",
    "\n",
    "\n",
    "\n",
    "$\\ex{2.1}$ First complete the `ratio()` function to compute $p$. The function should, given a list of boolean values as class labels, return the ratio of `True` labels in the list, e.g. $1.0$ would indicate the list only contained `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio for train set: 0.5311004784688995\n",
      "Ratio for validation set: 0.5444444444444444\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def ratio(labels):\n",
    "    tr = 0\n",
    "    for x in labels:\n",
    "        if x == True:\n",
    "            tr = tr + 1\n",
    "    return tr/len(labels)\n",
    "    \n",
    "# (expected ratio=~0.531)\n",
    "print('Ratio for train set:', ratio(y_train))\n",
    "# (expected ratio=~0.544)\n",
    "print('Ratio for validation set:', ratio(y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the entropy. Before we start writing the code, we deal with a possible source of error: The computation of $0\\ log_2(0)$ will correctly result in a math error.\n",
    "\n",
    "$\\ex{2.2}$ Complete the function `entropy_sub()` to compute the value of the log product, making sure to return $0$ in the case that $p$ is $0$ instead of an \n",
    "error. Then combine `ratio()` and `entropy_sub()` to compute the `entropy()` of a list of boolean class labels.\n",
    "\n",
    "**Hint:** Use Python's built in `math.log2()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for train set: 0.9972073335729776\n",
      "Entropy for validation set: 0.9942929346520454\n"
     ]
    }
   ],
   "source": [
    "def entropy_sub(p):\n",
    "    \"\"\"\n",
    "    Returns the value for p log_2(p)\n",
    "    \"\"\"\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return p*math.log2(p)\n",
    "    \n",
    "def entropy(labels):\n",
    "    res = 0;\n",
    "    \"\"\"\n",
    "    Returns the entropy of an array of labels, computed using equation (2.1.b)\n",
    "    \"\"\"\n",
    "    ratioo = ratio(labels)\n",
    "    return -entropy_sub(ratioo)-entropy_sub(1-ratioo)\n",
    "\n",
    "# (expected entropy=~0.997)\n",
    "print('Entropy for train set:', entropy(y_train))\n",
    "# (expected entropy=~0.994)\n",
    "print('Entropy for validation set:', entropy(y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropies of $s$ sets of labels can be combined using a weighted sum:\n",
    "\n",
    "(2.2) $$I_m = \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "which will give us the overall entropy ($I$ or information) of a split on variable $m$, where $N$ is the size of the set before the split, $N_j$ is the size of the $j^{th}$ set after the split, and $\\phi(p_j)$ is the entropy of the $j^{th}$ set.\n",
    "\n",
    "$\\ex{2.3}$ Complete the function `split_entropy()` to compute this value for a list of labels and `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the data before splitting: 1.0\n",
      "Worst case: 1.0\n",
      "Better: 0.8112781244591328\n",
      "Optimal: 0.0\n"
     ]
    }
   ],
   "source": [
    "def split_entropy(labels_list, N):\n",
    "    information = 0\n",
    "    for labels in labels_list:\n",
    "        information = information + (len(labels)/N)*entropy(labels) \n",
    "        pass\n",
    "        \n",
    "    return information\n",
    "\n",
    "labels = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "N = len(labels)\n",
    "print('Entropy of the data before splitting:', entropy(labels))\n",
    "\n",
    "# Worst case split (expected entropy=1.0)\n",
    "labels_list = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])\n",
    "print('Worst case:', split_entropy(labels_list, N))\n",
    "\n",
    "# Better split (expected entropy=~0.811)\n",
    "labels_list = np.array([[0, 0, 0, 1], [1, 1, 1, 0]])\n",
    "print('Better:', split_entropy(labels_list, N))\n",
    "\n",
    "# Perfect split (expected entropy=0.0)\n",
    "labels_list = np.array([[0, 0, 0, 0], [1, 1, 1, 1]])\n",
    "print('Optimal:', split_entropy(labels_list, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain (IG) measures how much the entropy changes by making a specific split, i.e. the relative gain in predictability of the data by making a specific distribution of labels. Information Gain is defined as the entropy of the original distribution $\\phi(p)$ minus the entropy of the split distribtution $I_m$, resulting from the split on variable $m$.\n",
    "\n",
    "$$IG_m = \\phi(p) - I_m$$\n",
    "\n",
    "The IG thus depends on two things: \n",
    "- the previous list of labels\n",
    "- how these labels are divided into new distributions by the split.\n",
    "\n",
    "$\\ex{2.4}$ Complete the `information_gain()` function using the earlier created functions `entropy()` and `split_entropy()`, with as input the previous list of labels (without a split) and a list of indices for each part of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for the \"better\" split: 0.18872187554086717\n"
     ]
    }
   ],
   "source": [
    "def information_gain(labels, indices): \n",
    "    labels_list = []\n",
    "    for index_list in indices:\n",
    "        labels_list.append(labels[index_list])\n",
    "    # STUDENT\n",
    "    gain = entropy(labels)-split_entropy(labels_list, len(labels))\n",
    "    return gain\n",
    "\n",
    "labels = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "labels_list = np.array([[0, 0, 0, 1], [1, 1, 1, 0]])\n",
    "\n",
    "# Now we create `indices` which correspond to the indices of the split (compare it with the two lists above). \n",
    "# You will have to write code that creates this list later.\n",
    "indices = np.array([[0, 1, 2, 4], [5, 6, 7, 3]])\n",
    "\n",
    "# IG for the \"better split\" (expected=~0.188)\n",
    "print('Information Gain for the \"better\" split:', information_gain(labels, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Decision Trees\n",
    "\n",
    "With the data turned into a usable format and the functions to measure entropy and IG ready, we can start with building the actual decision tree. Classes are a great way to represent trees: each DecisionTree object represents a node (or subtree) in the tree and we only have to store references to that node's children to build the structure of the tree. We will distinguish two types of 'nodes':\n",
    "\n",
    "1. DiscreteTree nodes, split on the basis of the value a discrete variable\n",
    "2. NumericTree nodes, split on the basis of the value of numeric variable\n",
    "\n",
    "Because these two classes are very similar, we will use *inheritance* to avoid redundancy. Here we will define a parent class `DecisionTree` that will contain all the common elements for `DiscreteTree` and `NumericTree`.\n",
    "\n",
    "We will use some less intuitive Python to actually create a general `DecisionTree` node and then turn it into `DiscreteTree` or `NumericTree` instance based on the split results. This will actually greatly simplify building a tree which can handle both numeric and discrete splits. The `DecisionTree` class will thus not only hold all the common functions used for both the `DiscreteTree` and `NumericTree` nodes, but we will also use its `__init__` method as generic constructor for either specific type of node. The code to pick the correct concrete class has already been provided, as well as most other logic, so you will only need to focus on completing the split functions for `DiscreteTree` and `NumericTree`.\n",
    "\n",
    "$\\ex{3.1}$ Start by reading the code that has been provided here, so you get a sense of the general structure, and most importantly, what the attributes for each `DecisionTree` nodes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, data_discrete, data_numeric, labels, tree_type=0, thres=0.1):\n",
    "        \"\"\" Creates a Decision Tree, based on the following arguments:\n",
    "                data_discrete - A 2D array of ints, each row containing the discrete features for a patient.\n",
    "                data_numeric - A 2D array of floats, each row containing the numeric features for a patient.\n",
    "                labels - An array of boolean class labels, each corresponding to a\n",
    "                        DataRow instance of a patient at the same index. \n",
    "                tree_type - 0: create the Tree with the highest IG every node \n",
    "                            1: create DiscreteTrees only\n",
    "                            2: create NumericTrees only\n",
    "                thres - The cutoff value for IG, to stop splitting the tree.\n",
    "                        Below this value the node becomes a leaf node and no\n",
    "                        further splits are made.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        # Store the basic attributes for any DecisionTree\n",
    "        self.data_discrete = data_discrete\n",
    "        self.data_numeric = data_numeric\n",
    "        self.labels = labels\n",
    "        self.tree_type = tree_type\n",
    "        self.thres = thres\n",
    "        \n",
    "        # Compute the current ratio of labels and assign this node the most common label\n",
    "        self.ratio = ratio(self.labels)\n",
    "        # This will assign a boolean value to self.label, as `self.ratio >= 0.5` is a boolean statement\n",
    "        self.label = self.ratio >= 0.5\n",
    "        \n",
    "        if self.tree_type == 1:\n",
    "            # Convert this DecisionTree to a DiscreteTree and perform the split\n",
    "            discr_tree = DiscreteTree(self)\n",
    "            self.convert_tree(discr_tree)\n",
    "        elif self.tree_type == 2:\n",
    "            # Convert this DecisionTree to a NumericTree and perform the split\n",
    "            numer_tree = NumericTree(self)\n",
    "            self.convert_tree(numer_tree)\n",
    "        else:\n",
    "            # If no specific type has been given (tree_type: 0) we determine which type is best\n",
    "            # by computing both options and comparing the information gain.\n",
    "            # Create a DiscreteTree and NumericTree, passing all the stored attributes\n",
    "            # as an argument, and compute the best possible split for each\n",
    "            discr_tree = DiscreteTree(self)\n",
    "            numer_tree = NumericTree(self)\n",
    "            \n",
    "            # Based on the results of the split computations, replace this generic\n",
    "            # DecisionTree node with either a DiscreteTree or a NumericTree node\n",
    "            if discr_tree.info_gain > numer_tree.info_gain:\n",
    "                self.convert_tree(discr_tree)\n",
    "            else:\n",
    "                self.convert_tree(numer_tree)\n",
    "        \n",
    "        # Create an empty dictionary to contain the (possible) branches from this node,\n",
    "        # where the values should be new DecisionTree nodes, or None if not present\n",
    "        self.branches = defaultdict(lambda: None)\n",
    "        \n",
    "        # Check if this split produced a high enough Information Gain to actually create\n",
    "        # the resulting branches with new split nodes below it,\n",
    "        # else no split is carried out and the original node is a leaf node\n",
    "        self.leaf = self.info_gain < self.thres\n",
    "        if not self.leaf:\n",
    "            self.create_subtrees()\n",
    "    \n",
    "    def store_split_values(self, feat_index, feat_values, indices, info_gain):\n",
    "        \"\"\" Stores the values of the passed parameters as object attributes. Is intended\n",
    "            to store the results of a split computation for either a DiscreteTree or a\n",
    "            NumericTree. The stored attributes are:\n",
    "                feat_index - The index of the feature on which the split was\n",
    "                    based.\n",
    "                feat_values - A list of the possible values that this split feature can\n",
    "                    take, each corresponding to a different branch in the DecisionTree\n",
    "                indices - A list of index lists, with each list containing the indices\n",
    "                    defining a subset of the current data and label attributes, as\n",
    "                    computed by the split. The order of these subsets should match the\n",
    "                    order of the corresponding feat_values used to define the branches\n",
    "                    of the split.\n",
    "                info_gain - Information Gain computed for this split\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.feat_index = feat_index\n",
    "        self.feat_values = feat_values\n",
    "        self.indices = indices\n",
    "        self.info_gain = info_gain\n",
    "    \n",
    "    def convert_tree(self, new_tree):\n",
    "        \"\"\" Converts this object to the tree passed as the new_tree parameter.\n",
    "            All attributes from the new_tree are transfered.\n",
    "                new_tree - Either a DiscreteTree or a NumericTree instance, to which\n",
    "                            this object is converted\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__class__ = new_tree.__class__\n",
    "        self.__dict__ = new_tree.__dict__\n",
    "    \n",
    "    def create_subtrees(self):\n",
    "        \"\"\" Creates the different subsets of the current data and labels, and makes a\n",
    "            a new DecisionTree node for each such subset, based on the indices attribute\n",
    "            stored after the computed split. These new DecisionTrees are stored in the \n",
    "            branches attribute, a dictionary mapping the value of a variable from the\n",
    "            split to the new DecisionTree created by selecting that value for the split.\"\"\"\n",
    "        for i, key in enumerate(self.feat_values):\n",
    "            subset_discrete = self.data_discrete[self.indices[i]]\n",
    "            subset_numeric = self.data_numeric[self.indices[i]]\n",
    "            subset_labels = self.labels[self.indices[i]]\n",
    "            subtree = DecisionTree(subset_discrete, subset_numeric, subset_labels, tree_type = self.tree_type)\n",
    "            self.branches[key] = subtree\n",
    "        \n",
    "    def classify(self, row_discrete, row_numeric):\n",
    "        \"\"\" Traverses the DecisionTree based on the values stored in the given row and\n",
    "            returns the most common label in the resulting leaf node.\n",
    "                row - The index of the row being classified\"\"\"\n",
    "        # Option 1: node is a leaf\n",
    "        if self.leaf:\n",
    "            return self.label\n",
    "        \n",
    "        subtree = self.get_subtree(row_discrete, row_numeric)\n",
    "        \n",
    "        # Option 2: no valid subtree\n",
    "        if subtree is None:\n",
    "            return self.label\n",
    "        \n",
    "        # Option 3: there is a valid subtree\n",
    "        return subtree.classify(row_discrete, row_numeric)\n",
    "        \n",
    "    def split(self):\n",
    "        \"\"\" Must be implemented by the subclass based on the specific type of split performed.\n",
    "            The function here is only to ensure it is implemented, and should not be modified.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_subtree(self, instance):\n",
    "        \"\"\" Must be implemented by the subclass based on the specific type of split performed.\n",
    "            The function here is only to ensure it is implemented, and should not be modified.\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete split\n",
    "\n",
    "Now we can start with actually writing the `split()` function for the `DiscreteTree`. This function should, for every discrete variable in the data, try to create a split based on that variable and compute the *Information Gain* of the resulting split.\n",
    "\n",
    "For discrete splits, we split the set into a subset for each discrete label. For example, if there are three possible labels for a certain feature, we should return three subsets. The question is: which feature should we pick to split on. You can find out by performing the following steps:\n",
    "- For each feature:\n",
    "    1. Split the set into subsets corresponding to each discrete label.\n",
    "    2. Compute the information gain for this split.\n",
    "- Then: split the dataset based on the feature with the highest information gain.\n",
    "\n",
    "Once the best feature for the split has been determined, the results of the split need to be stored in the instance, so they can be used to build the rest of the tree.\n",
    "\n",
    "Let's assume the algorithm decided to split on `chest pain type` (#9). The following attributes should be stored when splitting:\n",
    "\n",
    "1. The index of the feature to split on (e.g. 11)\n",
    "2. A list of discrete options for this feature (e.g. [0, 1, 2, 3], indicating the type of chest pain)\n",
    "3. A list of indices per option, so the first sublist contains the indices of all rows with chest pain type = 0, etc. (e.g [[0, 3, 4, 5, 7, 9, ...], [8, ...], [2, 6, ...], [1, ...]])\n",
    "4. The Information Gain resulting from the split (e.g. 0.8)\n",
    "\n",
    "These attributes can then be used to build the rest of the tree.\n",
    "\n",
    "$\\ex{3.2}$ Write the `create_indices_list()` function following the steps pointed out above.\n",
    "\n",
    "__Hint__: You can use `np.unique()` to get the unique values in a list.\n",
    "\n",
    "$\\ex{3.3}$ Complete the `split()` function following the steps pointed out above.\n",
    "\n",
    "__Hint__: Remember to reference attributes of a class using `self`, e.g. `self.data_discrete` instead of `data_discrete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_list(column):\n",
    "    \"\"\" Creates the indices list, containing for each possible value of the current feature, \n",
    "        the indices of corresponding rows (e.g. [[0, 2], [1, 3], ...] where the current\n",
    "        feature is 0 in rows 0 and 2).\n",
    "        Returns the list of indices and as second output a list of all possible feature values.\n",
    "            column - The column of one feature from the data.\"\"\"\n",
    "    \n",
    "    # STUDENT\n",
    "    pass\n",
    "    \n",
    "\n",
    "vals = np.array([0, 0, 1, 0, 1, 1, 2, 2])\n",
    "\n",
    "# indices_list expected is [[0, 1, 3], [2, 4, 5], [6, 7]],\n",
    "# feat_values expected is [0, 1, 2].\n",
    "indices_list, feat_values = create_indices_list(vals)\n",
    "\n",
    "print(indices_list)\n",
    "print(feat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            discrete variable to split this subset of the data on.\n",
    "                dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                        DiscreteTree instance.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best discrete variable to split the current dataset on,\n",
    "            based on the IG resulting from the split. For this best split variable, the\n",
    "            function stores several resulting attributes from the split, using the\n",
    "            store_split_values function. See the documentation of store_split_values\n",
    "            for an overview of what should be stored.\"\"\"\n",
    "        max_feat = None\n",
    "        max_feat_values = None\n",
    "        max_split = None\n",
    "        max_ig = 0\n",
    "        \n",
    "        for feat in range(self.data_discrete.shape[1]):\n",
    "            # 1. Call create_indices_list() for the feature column.\n",
    "            # 2. Compute the IG of the split\n",
    "            # 3. If IG > max IG, update max values\n",
    "            \n",
    "            # STUDENT\n",
    "            pass\n",
    "            \n",
    "        self.store_split_values(max_feat, max_feat_values, max_split, max_ig)\n",
    "            \n",
    "    def get_subtree(self, row_discrete, row_numeric):\n",
    "        \"\"\" Returns the subtree one branch down.\n",
    "            Returns None if the value was not present at the split.\n",
    "                row_discrete - array of the discrete values\n",
    "                row_numeric - array of the numeric values\"\"\"\n",
    "        value = row_discrete[self.feat_index]\n",
    "        return self.branches.get(value, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the subtrees\n",
    "\n",
    "If we were to repeat this process of splitting each node, we end up with a tree structure. But we need to stop at a certain moment to avoid building infinite trees and to avoid overfitting.\n",
    "\n",
    "There are quite a few strategies to decide when to stop. The simplest of these is just to stop splitting when the *Information Gain* of a split drops below a certain threshold.\n",
    "\n",
    "This is already implemented as the last step in the `__init__` function of `DecisionTree`. The computed *Information Gain* is compared to the threshold and if the gain is too low, the node is labeled as a leaf node. If the node is not a leaf node, then further splits should be attemped and the `create_subtrees()` function is called to populate the branches of this node with new subset `DecisionTrees`.\n",
    "\n",
    "$\\ex{3.4}$ Take out a pen and paper and try to draw the structure that would be built if you created a new `DecisionTree` using the training data. No need to do the actual entropy math on paper, just try and sketch what objects would be created and how they would relate to each other. If you are having trouble visualizing this for such a large dataset, take a look at the small example tree below.\n",
    "\n",
    "<img src=\"images/decision-tree.png\" alt=\"Decision Tree\" style=\"height: 300px;\"/>\n",
    "\n",
    "*Image retrieved from: Alpaydin, E. (2010). Introduction to machine learning. Cambridge, Mass.: Mit Press.*\n",
    "\n",
    "$\\ex{3.5}$ Take a look at the `create_subtrees()` function (in the `DecisionTree` class above) and try to understand how it works.\n",
    "\n",
    "__Note:__ In order to traverse the decision tree, each new `DecisionTree` should be stored in the `branches` dictionary of the node. Here the *key* should be the value of the discrete variable for that split (stored in `self.feat_values`) and the *value* should be the new `DecisionTree` resulting from that choice in the split. When this dictionary is complete, each key-value pair will represent a different branch of the tree at that split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the subtrees and classifying patient records\n",
    "\n",
    "The tree building part of the algorithm is actually complete now! With this whole structure built, actually classifying a new patient record is pretty easy. In the code, we refer to a patient record as a row. The patient record is split into a discrete and numeric part: `row_discrete` and `row_numeric`.\n",
    "\n",
    "$\\ex{3.6}$ Take a look at the `get_subtree()` function in `DiscreteTree`, which should return the `DecisionTree` that corresponds to the value of `row_discrete` at the split feature.\n",
    "\n",
    "The previous exercise gives us a way to traverse the tree, given a patient record. Now, we want to classify the patient record by traversing the tree. At each DecisionTree node, we have three options, based on the type of node:\n",
    "\n",
    "1. The node is a leaf node, in which case the classification will be the most common label of that node\n",
    "2. The node does not have a valid subtree for the splitted value, so the classification will also be the node label\n",
    "3. The node does have subtree for the splitted value, in which case you can recursively continue classifying on the subtree\n",
    "\n",
    "$\\ex{3.7}$ Take a look at the `classify()` function in `DecisionTree`, which should classify a patient record consisting of an array of discrete values and an array numeric values using the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating patient records\n",
    "\n",
    "$\\ex{3.8}$ Finally, write a `validate()` function which should take a trained decision tree, a validation set of patient records and corresponding labels and returns the percentage which was classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(decision_tree, data_discrete, data_numeric, labels):\n",
    "    \"\"\" Classifies all patient records and compares the outcome to \n",
    "        the provided labels. Returns the percentage of elements that was classified\n",
    "        correctly.\n",
    "            data_discrete - A 2D array of ints, each row containing the discrete features for a patient.\n",
    "            data_numeric - A 2D array of floats, each row containing the numeric features for a patient.\n",
    "            labels - List of boolean labels each belonging to a patient record\"\"\"\n",
    "    # STUDENT\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{3.9}$ Create a `DecisionTree` using the training data, with `tree_type` set to $1$ (currently we can only do discrete splits) and print the results when validating with the validation set created earlier. You might get a `RuntimeWarning` which you can ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_decision_tree = None # STUDENT\n",
    "\n",
    "# (expected validation=~0.767)\n",
    "validate(trained_decision_tree, x_validation_disc, x_validation_num, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding the NumericTree\n",
    "\n",
    "Now we move on to adding the numeric splits to the tree. All the code already written in the `DecisionTree` class, can just be inherited down to the `NumericTree` class as well, meaning you will only need to write the 2 numeric-specific functions.\n",
    "\n",
    "The most important of these function is the `split()` function. The numeric split is based on a split boundary, where all values smaller than the boundary go in one branch, and those greater or equal go in the other branch.\n",
    "\n",
    "$\\ex{4.1}$ Complete the `split()` function in the `NumericTree` class below. This function tries every possible split boundary for every feature and uses the split with the best IG overall. Try and come up with a logical way to generate all possible ways to separate a set of numeric values into two sets using a split boundary.\n",
    "\n",
    "__Hint 1__: You don't have to iterate over *all* possible split values. You could also just iterate over all values that are present in the dataset, since the values in between make no difference when splitting the data.\n",
    "\n",
    "__Hint 2__: As a last addition, you should also store the split boundary for this best split, as you will need to compare new values to this same boundary in order to classify them. You can create a new object value, like `self.boundary`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{4.2}$ Take a look at the `get_subtree()` function in the `NumericTree` class below, which compares the value of the variable on which the split was performed to the split boundary. It uses this to get the correct subtree from the `branches` attribute, where *False* is for the smaller values and *True* for those greater or equal, as described in the section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data, labels):\n",
    "    max_feat = None\n",
    "    max_split = None\n",
    "    max_ig = 0\n",
    "    max_boundary = None\n",
    "\n",
    "    for feat in range(data.shape[1]):\n",
    "        col = data[:, feat]\n",
    "\n",
    "        for curr_boundary in col:\n",
    "            # STUDENT\n",
    "            pass\n",
    "    return max_feat, max_split, max_ig, max_boundary\n",
    "\n",
    "max_feat, max_split, max_ig, max_boundary = find_best_split(x_train_num[:10,:], y_train[:10])\n",
    "\n",
    "print('Best split:')\n",
    "\n",
    "# expected feature index=3\n",
    "print('\\tFeature index:', max_feat)\n",
    "\n",
    "# expected split=[[1, 3, 5, 6, 7], [0, 2, 4, 8, 9]]\n",
    "print('\\n\\tSplit:', max_split)\n",
    "\n",
    "# expected IG=~0.278\n",
    "print('\\n\\tIG:', max_ig)\n",
    "\n",
    "# expected boundary=159.0\n",
    "print('\\n\\tBoundary:', max_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            numeric variable to split this subset of the data on.\n",
    "                dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                        NumericTree instance.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best boundary for any numeric variable to split the\n",
    "            current dataset on, based on the IG resulting from the split. For this\n",
    "            best split boundary, the function stores several resulting attributes\n",
    "            from the split, using the store_split_values function. See the\n",
    "            documentation of store_split_values for an overview of what should\n",
    "            be stored. In addition, one more attribute is stored in the numeric\n",
    "            case, namely the boundary value used for the split.\"\"\"\n",
    "        max_feat, max_split, max_ig, boundary = find_best_split(self.data_numeric, self.labels)\n",
    "        self.boundary = boundary\n",
    "        \n",
    "        max_feat_values = [False, True]\n",
    "        self.store_split_values(max_feat, max_feat_values, max_split, max_ig)\n",
    "        \n",
    "    def get_subtree(self, row_discrete, row_numeric):\n",
    "        \"\"\" Returns the subtree one branch down.\n",
    "                row_discrete - array of the discrete values\n",
    "                row_numeric - array of the numeric values\"\"\"\n",
    "        value = row_numeric[self.feat_index] >= self.boundary\n",
    "        return self.branches.get(value, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing the results\n",
    "\n",
    "Now, we can create the actual decision tree.\n",
    "\n",
    "Create three `DecisionTree` instances using the training data, with `tree_type` set to $0$, $1$ and $2$. Tree type 1 will contain only discrete splits, tree type 2 will contain only numeric splits and tree type 0 will try both and use the split with the highest information gain. Print the average validation results for all three trees using the validation set created earlier. Repeat this process for multiple splits of the test set.\n",
    "\n",
    "Do not forget to pass `threshold` when creating a `DecisionTree`, because this parameter will be used in the next exercise.\n",
    "\n",
    "Note that the hybrid tree might not be the most accurate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(threshold = 0.1, n_iterations = 50):\n",
    "    hybrid_accuracy = 0\n",
    "    discrete_accuracy = 0\n",
    "    numeric_accuracy = 0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, None)\n",
    "        \n",
    "        # STUDENT\n",
    "        pass\n",
    "\n",
    "    hybrid_accuracy /= n_iterations\n",
    "    discrete_accuracy /= n_iterations\n",
    "    numeric_accuracy /= n_iterations\n",
    "    \n",
    "    return hybrid_accuracy, discrete_accuracy, numeric_accuracy\n",
    "\n",
    "hybrid, discrete, numeric = get_accuracy(0.1)\n",
    "\n",
    "# Note: all expected values are +-0.05\n",
    "\n",
    "# (expected hybrid accuracy=~0.744)\n",
    "print('Hybrid accuracy:', hybrid)\n",
    "# (expected discrete accuracy=~0.779)\n",
    "print('Discrete accuracy:', discrete)\n",
    "# (expected numeric accuracy=~0.695)\n",
    "print('Numeric accuracy:', numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate the importance of a good threshold value, we are going to plot the accuracy for different threshold values. Note that the code below might run for a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.05, 0.25, 10)\n",
    "\n",
    "accuracies_hybrid = []\n",
    "accuracies_discrete = []\n",
    "accuracies_numeric = []\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    hybrid, discrete, numeric = get_accuracy(threshold, n_iterations=1)\n",
    "    accuracies_hybrid.append(hybrid)\n",
    "    accuracies_discrete.append(discrete)\n",
    "    accuracies_numeric.append(numeric)\n",
    "    \n",
    "_, axis = plt.subplots()\n",
    "axis.plot(threshold_values, accuracies_hybrid, label = 'hybrid')\n",
    "axis.plot(threshold_values, accuracies_discrete, label = 'discrete')\n",
    "axis.plot(threshold_values, accuracies_numeric, label = 'numeric')\n",
    "\n",
    "axis.set_xlabel('Threshold')\n",
    "axis.set_ylabel('Accuracy')\n",
    "\n",
    "axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis\n",
    "\n",
    "If your algorithm is correct and you averaged over enough different validation splits, you might see some strange results in the comparison you just produced. For the last part of the assignment, answer these questions about the results.\n",
    "\n",
    "$\\q{6.1}$ Can you explain how it is possible that the validation score using just the discrete variables is higher than the validation score using the discrete and numeric variables combined? What property of the algorithm makes this outcome possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{6.2}$ What is your hypothesis for why this happens for this particular data? What could you do to improve the validation results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
