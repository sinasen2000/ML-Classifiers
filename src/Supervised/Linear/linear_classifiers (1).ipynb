{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Linear Classifiers\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma and Gosia Migut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions. \n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of linear classifiers and to get familiar with a benchmark machine learning problem: MNIST.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. There is quite a bit of theory and explanation in these notebooks. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/town-square\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$\n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^{\\top}}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying hand written digits with logistic regression\n",
    "\n",
    "In this assignment, you will try your hand at a common Machine Learning problem: classifying hand-written digits. Consider a postal office, where each mail item needs to be sorted according to its zip code. Of course, this zip code was handwritten. The code that you will write in the coming exercises will be able to bridge the gap between some scribbles on an envelope and actual digits a computer can understand.\n",
    "\n",
    "We already have a large set of digits: the MNIST dataset. Originally created by the National Institute of Standards and Technology (NIST) as the NIST dataset with digits written by postal office workers, it was modified by adding digits written by high school students to become the Modified NIST dataset (MNIST). MNIST consists of 60.000 training samples and 10.000 testing samples, where each sample is a $28\\times28$ grayscale image of a digit ($0 - 9$).\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "$\\rightarrow$ Make you sure that you have installed `scipy` and `scikit-learn` in your conda environment. If you're unsure that you have, run `conda list` in your terminal to check which packages are installed and use `conda install [package name]` to install new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command imports numpy and matplotlib and makes sure all plots are displayed inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying MNIST with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn, the machine learning package for Python, has the MNIST dataset built in. The dataset that we use contains images of hand-written digits that are only 8 by 8 pixels (instead of the original $28 \\times 28$ pixels), which means the algorithm (logistic regression) should run on every computer.\n",
    "\n",
    "The code in the following cell shows how to work with the digits dataset and how to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load function for the dataset\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits with 10 classes (0 - 9)\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "# Plot the numbers in two rows\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "white_line = np.ones((1, 40)) * 16\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "# Show both rows at the same time\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "plt.imshow(np.vstack((firstrow, white_line, secondrow)))\n",
    "\n",
    "print(\"The numbers shown are:\", np.vstack((digits.target[:5], digits.target[5:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression for classification\n",
    "\n",
    "Let's reformulate the goal for this exercise into mathematical notation:\n",
    "\n",
    "- We have $m$ training images $x^{(1)}, \\ldots, x^{(m)}$. Each $x^{(i)}$ is a $64 \\times 1$ vector, represting the grayscale values for each pixel in the $8 \\times 8$ image.\n",
    "- Each image is associated with a discrete label $y^{(i)} \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$\n",
    "- We want to derive a hypothesis function that can predict the label of any new image $x$. We will write this hypothesis function as $h_{\\theta}(x) \\approx y$\n",
    "\n",
    "In this notebook, we will find out the following:\n",
    "1. How we define $h_{\\theta}(x)$\n",
    "2. How we define a 'good' classifier with log likelihood\n",
    "3. How to find a $\\v\\theta$ that maximizes the log likelihood\n",
    "\n",
    "And finally, we will apply these lessons to classify MNIST.\n",
    "\n",
    "### 1. Binary classification using probilities\n",
    "\n",
    "Let's start simple, by only performing binary classification: Given a digit, we want to find out if it is a $1$ or a $0$. In other words: if it is a 1 or not.\n",
    "\n",
    "We will setup the hypothesis function to predict the probability that the given digit is a 1. If the probability is high, we can conclude the digit is a 1, if it is low, the digit is likely to be a 0.\n",
    "\n",
    "Thus, we set:\n",
    "\n",
    "$$\n",
    "P(y = 1 | x; \\theta) = h_{\\theta}(x)\\\\\n",
    "P(y = 0 | x; \\theta) = 1 - h_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "Now, it's time to formulate the hypothesis function. The hypothesis function should take as input the image $x$, which is a vector of length $64$ and a vector $\\theta$, which we will learn. We define the following hypothesis function:\n",
    "\n",
    "(eq. 1.1)$$\n",
    "h_{\\v\\theta}(x) = g(\\v\\theta\\T x) = \\frac{1}{1 + e^{-\\v\\theta\\T x}}\n",
    "$$\n",
    "\n",
    "Let's take a look at why we use this formulation. First, the dot product $\\v\\theta\\T x$:\n",
    "\n",
    "$\\q{1.1}$ Write out the dot-product between $\\theta$ and $x$ if both are a vector of size $4$. Can you see that the dot product is a simple way of writing a linear combination of $x$'s elements?\n",
    "\n",
    "Next, the function around the dot product, $g(z)$, which we call the sigmoid function:\n",
    "\n",
    "(eq. 1.2)$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Let's plot it to see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "plot(z, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $g(z)$ tends towards $1$ as $z \\rightarrow \\infty$, and $g(z)$ tends towards $0$ as $z \\rightarrow - \\infty$. Also, $g(z)$, and therefore $h(x)$, is always bounded between $0$ and $1$.\n",
    "\n",
    "$\\q{1.2}$ Why are these properties convenient to model a probability?\n",
    "\n",
    "A useful property of the derivative of the sigmoid function that we will use later on is the following:\n",
    "\n",
    "$$\n",
    "      g'(z) = g(z)(1 - g(z))\n",
    "$$\n",
    "\n",
    "$\\ex{1.1}$ Implement the hypothesis function from eq. 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(x,theta):\n",
    "    \"\"\"\n",
    "    Computes the hypothesis function.\n",
    "    :param x: numpy array of size (d, ) where d are the number of input features\n",
    "    :param theta: numpy array of size (d, n) of theta where d is the number of input features \n",
    "        and n is the number of classifiers.\n",
    "    :return: predicted probability.\n",
    "    \"\"\"\n",
    "    sigmoid = None\n",
    "    # STUDENT\n",
    "    return sigmoid\n",
    "\n",
    "# The images are stored in digits.images as 8 x 8 arrays.\n",
    "# Therefore, we flatten the image to a 64 x 1 array\n",
    "x = digits.images[0].flatten()\n",
    "\n",
    "# To test our hypothesis function, we set three different theta vectors\n",
    "# All 1\n",
    "theta_ones = np.ones(64)\n",
    "# All 0\n",
    "theta_zeros = np.zeros(64)\n",
    "# All -1\n",
    "theta_min_ones = -1 * np.ones(64)\n",
    "\n",
    "# And apply the prediction\n",
    "hypothesis_ones = hypothesis(x, theta_ones)\n",
    "hypothesis_zeros = hypothesis(x, theta_zeros)\n",
    "hypothesis_min_ones = hypothesis(x, theta_min_ones)\n",
    "\n",
    "# Output for each theta vector\n",
    "print(\"Prediction ones: {}\".format(hypothesis_ones))\n",
    "print(\"Prediction zeros: {}\".format(hypothesis_zeros))\n",
    "print(\"Prediction minus ones: {}\".format(hypothesis_min_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.3}$ We have supplied three different variants for $\\theta$ and computed the prediction for each. Can you explain the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Probabilities\n",
    "Remember that we wanted to use our hypothesis function to predict the likelihood of a label given the input image:\n",
    "\n",
    "$$\n",
    "P(y = 1 | x; \\theta) = h_{\\theta}(x)\\\\\n",
    "P(y = 0 | x; \\theta) = 1 - h_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "We can rewrite these two equations into one:\n",
    "\n",
    "(eq. 2.1) $$\n",
    "p(y | x; \\theta) = (h_{\\theta}(x))^y(1 - h_{\\theta}(x))^{1-y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "Our next question is the following: how do we set $\\theta$ to a good value. Before we can answer these questions, though, we need to define what is 'good'.\n",
    "\n",
    "We will set out the following goal for our classifier: **given an image $x^{(i)}$ and its label $y^{(i)}$, we want the predicted probability for the correct label ($y^{(i)}$) to approach $1$**:\n",
    "\n",
    "(eq. 2.2)$$\n",
    "p(y^{(i)} | x^{(i)}; \\theta) \\rightarrow 1\n",
    "$$\n",
    "\n",
    "$\\q{2.1}$ Write eq. 2.2 using the hypothesis function (eq. 2.1) instead of the probability notation.\n",
    "\n",
    "The predicted probability for the correct labels is called the _likelihood_ of our classifier. To make it easier to train our classifier, we will try to maximize the likelood instead of desiring the probability to approach $1$.\n",
    "\n",
    "We can compute the likelihood of the entire training set, by computing the product of the likelihood of all samples in the training set:\n",
    "\n",
    "(eq. 2.3)$$\n",
    "L(\\theta) = p(\\mathbf{y} | X; \\theta) = \\prod_{i = 1}^m p(y^{(i)} | x^{(i)}; \\theta)\n",
    "$$\n",
    "\n",
    "$\\q{2.2}$ Substitute [eq. 2.1](#Probabilities) into the likelihood for the entire training set.\n",
    "\n",
    "To recap our goal for an entire training set: **given a training set $X$ with labels $\\mathbf{y}$, we want to maximize the likelihood.**\n",
    "\n",
    "A final step before we can start implementing. It is easier to maximize the log of the likelihood. We call this the _log likelihood_. This results in the following formulation for the _log likelihood_ of our parameters:\n",
    "\n",
    "(eq. 2.4)$$\n",
    "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i = 1}^m y^{(i)} \\log (h_{\\theta}(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_{\\theta}(x^{(i)}))\n",
    "$$\n",
    "\n",
    "$\\q{2.3}$ Check to see if this is indeed the $\\log$ of your answer to question 2.2.\n",
    "\n",
    "$\\ex{2.1}$ Now implement eq 2.4. You are given a numpy array of predicted probabilities `x_predict` and a numpy array of correct labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(x_predict,y):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of your classifier.\n",
    "    :param x_predict: numpy array of predicted probabilities.\n",
    "    :param y: numpy array of actual y values.\n",
    "    :return: The log likelihood as a scaler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT\n",
    "\n",
    "# There will be errors from numpy regarding division by zero and invalid value in multiply. \n",
    "# If you wish to ignore these errors you can uncomment the following line:\n",
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# These predictions should do well\n",
    "x2_predict = np.array([0.2, 0.1, 0.9, 0.8])\n",
    "y2 = np.array([0, 0, 1, 1])\n",
    "print(likelihood(x2_predict, y2)) \n",
    "\n",
    "# These predictions are wrong\n",
    "x2_predict = np.array([0.9, 0.8, 0.99, 0.3, 0.1])\n",
    "y2 = np.array([0, 0, 1, 1, 1])\n",
    "print(likelihood(x2_predict, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Ascent\n",
    "\n",
    "In the lectures you were introduced to gradient descent, a method to find the (local) minimum of a function.\n",
    "\n",
    "$\\q{3.1}$ Explain the intuition behind gradient descent in your own words or illustrate the concept.\n",
    "\n",
    "Remember that we want to _maximize the log likelihood_. Maximizing instead of minimizing a function. Therefore, we will reuse the gradient descent theory and simply apply it in the reverse direction. This is what that looks like:\n",
    "1. Set $\\theta$ to a vector of random values.\n",
    "2. For each training sample:\n",
    "    * Compute the gradient of the _log likelihood_ function for that $\\theta$ and training sample.\n",
    "    * Adjust $\\theta$ in the direction of the gradient.\n",
    "3. Repeat from step 2 until convergence.\n",
    "\n",
    "$\\q{3.2}$ Which of these steps has been reversed with regards to gradient descent?\n",
    "\n",
    "For step 2, we will need the gradient of the _log likelihood_ function. We will provide this for you:\n",
    "\n",
    "(eq. 3.1)$$\n",
    "\\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta) = (y - h_{\\theta}(x))x_j\n",
    "$$\n",
    "\n",
    "$\\q{3.3 (optional)}$ Validate this derivative by computing the derivation of $\\ell(\\theta)$ yourself. \n",
    "__Hint__ remember the useful property of the sigmoid function? Also remember the chain rule.\n",
    "\n",
    "In step 2, we adjust $\\theta$ in the direction of the gradient multiplied by the learning rate. We are going to write this adjustment for each separate paremeter, $\\theta_j$:\n",
    "\n",
    "(eq. 3.2)$$\n",
    "\\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta)\\\\\n",
    "\\theta_j := \\theta_j + \\alpha(y^{(i)} - h_{\\theta}(x^{(i)}))x_j^{(i)}\n",
    "$$\n",
    "Where $\\alpha$ is the learning rate and $x_j^{(i)}$ is the $j$-th pixel in image $i$.\n",
    "\n",
    "$\\q{3.4}$ Why do we need the learning rate?\n",
    "\n",
    "$\\ex{3.1}$ Implement the code to apply the gradient ascent step (eq. 3.2). Try doing this without writing explicit `for` loops. Use Numpy to make computations for the entire array of values at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient(theta, x, y, alpha):\n",
    "    \"\"\"\n",
    "    Applies the gradient step to theta and returns an adjusted theta.\n",
    "    :param theta: current theta array of size (d,)\n",
    "    :param x: image array of size (d,)\n",
    "    :param y: integer label of image\n",
    "    :param alpha: learning rate float\n",
    "    :return: the updated theta array of size (d,)\n",
    "    \"\"\"\n",
    "    # STUDENT\n",
    "    return updated_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression on MNIST\n",
    "\n",
    "Great! We have a way to predict the probability for two classes. Now we will implement this for MNIST.\n",
    "\n",
    "First, we want to be able to make predictions for a test set. Before you write this, think about how you can transform a continuous probability into a discrete label.\n",
    "\n",
    "$\\ex{4.1}$ Write a predict function that gives the predictions for an entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary(x_test, theta):\n",
    "    \"\"\"\n",
    "    Predicts a label for each image in x_test using theta.\n",
    "    :param x_test: an array of size (m, 64) of all test images.\n",
    "    :param theta: a (64,) array of trained theta.\n",
    "    :return: an array of size (m,) of labels for each test_image.\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(x_test.shape[0])\n",
    "    for i, x in enumerate(x_test):\n",
    "        # STUDENT\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of predictions, we would like to compute the accuracy of these predictions.\n",
    "\n",
    "$\\ex{4.2}$ Complete the `accuracy` function, which returns the percentage of predictions that were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the predictions based on the true labels.\n",
    "    :param predictions: an array of size (m,) of the computed predictions for each image.\n",
    "    :param y_true: an array of size (m,) of the true labels of each image.\n",
    "    :return: the accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    # STUDENT\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{4.3}$ Complete the code below to run your classifier and predict the labels for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only ones and twos\n",
    "digits4 = load_digits(n_class=2)\n",
    "\n",
    "# Split dataset into train and test set\n",
    "x4_train = np.reshape(digits4.images[:240],(240,64))\n",
    "x4_test = np.reshape(digits4.images[240:],(120,64))\n",
    "y4_train = digits4.target[:240]\n",
    "y4_test = digits4.target[240:]\n",
    "\n",
    "# Set learning rate (try experimenting with this)\n",
    "alpha4 = 0.01\n",
    "\n",
    "# Set theta to intial value\n",
    "theta4 = np.zeros((64,))\n",
    "\n",
    "# We go through the entire training set a number of times\n",
    "# Each of these iterations is called an epoch\n",
    "n_epochs4 = 5\n",
    "for epoch in range(n_epochs4):\n",
    "    for i, x in enumerate(x4_train):\n",
    "        # Update theta\n",
    "        # STUDENT\n",
    "        \n",
    "# Next, we want to test our theta on the test set\n",
    "# Make predictions\n",
    "# STUDENT\n",
    "\n",
    "# And print the accuracy\n",
    "accuracy4 = compute_accuracy(predictions4, y4_test)\n",
    "print('Accuracy on binary classification: {}'.format(accuracy4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try setting the learning rate to a higher value and a lower value (e.g.: 0.05, 0.01 respectively).\n",
    "\n",
    "$\\q{4.1}$ Which learning rate performs better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{4.4}$ Complete the following code to see how the classifier develops over time. This plot resembles a learning curve, but does not completely suffice as a learning curve as it is not averaged over multiple splits or epochs.\n",
    "\n",
    "$\\q{4.2}$ Experiment with the learning rate. What is the effect on the 'learning curve'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate (try experimenting with this)\n",
    "alpha4 = 0.001\n",
    "\n",
    "# Set theta to intial value\n",
    "theta4 = np.zeros((64,))\n",
    "\n",
    "# We go through the entire training set a number of times\n",
    "# Each of these iterations is called an epoch\n",
    "n_epochs4 = 1\n",
    "accuracies4 = []\n",
    "for _ in range(n_epochs4):\n",
    "    for i, x in enumerate(x4_train):\n",
    "        # Update theta, get predictions and compute accuracy\n",
    "        # STUDENT\n",
    "        \n",
    "plt.plot(np.arange(len(accuracies4)), accuracies4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-class Logistic Classifier\n",
    "\n",
    "Next, we will extend your code for binary classification to ten classes.\n",
    "\n",
    "$\\q{5.1}$ Before we go through the steps to make this work, think about how you could do this for yourself.\n",
    "\n",
    "Remember that we set the hypothesis function to give the probability that an image was a $1$.\n",
    "\n",
    "$$\n",
    "P(y = 1 | x; \\theta) = h_{\\theta}(x)\\\\\n",
    "P(y = 0 | x; \\theta) = 1 - h_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "Because the only other class was a $0$, we could conclude that an image was $0$ if the probability was low.\n",
    "\n",
    "Now, we create ten hypothesis functions (ten different theta's): one for each class. Each hypothesis tells us the probability that a given image belongs to the corresponding class. If the probability is low, it must belong to some other class.\n",
    "\n",
    "When we apply these ten hypothesis functions to an image, we get ten different probabilities.\n",
    "\n",
    "$\\q{5.2}$ Given ten probabilities, one for each class, how would you decide which class an image belongs to?\n",
    "\n",
    "$\\ex{5.1}$ Finish the predict function for ten classes.\n",
    "\n",
    "__Hint__ `np.argmax` can be used to return the index of the maximum value in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiclass(x_test, theta):\n",
    "    \"\"\"\n",
    "    Predicts a label for each image in x_test using theta.\n",
    "    :param x_test: an array of size (m, 64) of all test images.\n",
    "    :param theta: a (64,10) array of trained theta.\n",
    "    :return: an array of size (m,) of labels for each test_image.\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(x_test.shape[0])\n",
    "    for i, x in enumerate(x_test):\n",
    "        # STUDENT\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.2}$ Complete the code below to perform multi-class classification.\n",
    "\n",
    "__Hint__ You might have to rewrite your `hypothesis` or `apply_gradient` functions to work well with the extra dimension for theta $(64, 10)$ instead of $(64,)$. The extra dimension can result in a mis-match between the dimensions of, for example `x` and the hypothesis output. You can add a dimension to your array by using: `x.reshape(-1, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient2(theta, x, y, alpha):\n",
    "    \"\"\"\n",
    "    Applies the gradient step to theta and returns an adjusted theta.\n",
    "    :param theta: current theta matrix of size (d, n) where n is the number of classifiers.\n",
    "    :param x: image array of size (d,)\n",
    "    :param y: a hot one encoded label of image (n,) where n is the number of classifiers.\n",
    "    :param alpha: learning rate float\n",
    "    :return: the updated theta matrix of size (d, n)\n",
    "    \"\"\"\n",
    "    # STUDENT\n",
    "    return updated_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "# Load only ones and twos, because \n",
    "#  linear classifiers can only differentiate between two classes.\n",
    "# (i.e. \"ones\" and \"Not-ones\")\n",
    "digits5 = load_digits(n_class=10)\n",
    "\n",
    "# Split dataset into train and test set\n",
    "x5_train, x5_test, y5_train, y5_test = train_test_split(digits5.data, digits5.target, test_size=0.4) \n",
    "\n",
    "# Set learning rate (try experimenting with this)\n",
    "alpha5 = 0.01\n",
    "\n",
    "# Set theta to intial value\n",
    "theta5 = np.zeros((64, 10))\n",
    "\n",
    "# We go through the entire training set a number of times\n",
    "# Each of these iterations is called an epoch\n",
    "n_epochs5 = 5\n",
    "for epoch in range(n_epochs5):\n",
    "    for i, x in enumerate(x5_train):\n",
    "        # We transform the label to a one-hot encoding vector\n",
    "        # This is a vector of zeros with a one on the location of the label\n",
    "        # E.g. if the label is 3, the one-hot encoding is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "        y5 = np.zeros(10)\n",
    "        y5[y5_train[i]] = 1\n",
    "        # Update theta\n",
    "        # STUDENT\n",
    "        \n",
    "# Next, we want to test our theta on the test set\n",
    "# Make predictions\n",
    "# STUDENT\n",
    "\n",
    "# And print the accuracy\n",
    "accuracy5 = compute_accuracy(predictions5, y5_test)\n",
    "print('Accuracy on multi-class classification: {}'.format(accuracy5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analysis\n",
    "\n",
    "In the previous week, you have implemented K-Nearest Neighbours, an example of a non-parametric classifier. This week, you implemented a linear classifier. Let's see how they compare on the bias-variance trade-off.\n",
    "\n",
    "- Bias describes the error of the classifier.\n",
    "- Variance describes how much a classifier changes when the training set changes.\n",
    "\n",
    "Often, a classifier with high bias has a low variance and vice-versa. This is what we call the bias-variance trade-off.\n",
    "\n",
    "$\\q{6.1}$ Which classifier do you expect to have higher bias: K-NN or the Logistic Classifier?\n",
    "\n",
    "$\\q{6.2}$ Which classifier do you expect to have higher variance: K-NN or the Logistic Classifier?\n",
    "\n",
    "To give you an intuition about these concepts, without going into too much detail, we will investigate these two with Sklearn's implementation.\n",
    "\n",
    "First, bias:\n",
    "\n",
    "$\\ex{6.1}$ Run the code below a number of times, or write a loop to automatically compute the average accuracy over a couple of different train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load function for the dataset, the train/test split function and the classifiers\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the digits with 10 classes (0 - 9)\n",
    "digits = load_digits(n_class=10)\n",
    "x6 = digits.images\n",
    "y6 = digits.target\n",
    "\n",
    "# Split the dataset randomly between train and test\n",
    "x6_train, x6_test, y6_train, y6_test = train_test_split(x6, y6, test_size=0.3, random_state=42)\n",
    "# Flatten images\n",
    "x6_train, x6_test = x6_train.reshape(x6_train.shape[0], -1), x6_test.reshape(x6_test.shape[0], -1)\n",
    "\n",
    "# Train the classifiers (using the fit function)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3).fit(x6_train, y6_train)\n",
    "lr_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs').fit(x6_train, y6_train)\n",
    "\n",
    "# Return predictions\n",
    "knn_predictions = knn_clf.predict(x6_test)\n",
    "lr_predictions = lr_clf.predict(x6_test)\n",
    "\n",
    "# Compute and print accuracy\n",
    "knn_accuracy = compute_accuracy(knn_predictions, y6_test)\n",
    "lr_accuracy = compute_accuracy(lr_predictions, y6_test)\n",
    "print('Accuracy of k-nn: {}, accuracy for logistic regression: {}'.format(knn_accuracy, lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{6.3}$ Which classifier has higher bias?\n",
    "\n",
    "Next, variance. To investigate this, we will plot the decision boundary in a 2D plot of a dummy dataset, since a decision boundary is hard to plot for 64 features. This shows us what the classifier 'looks like'.\n",
    "\n",
    "$\\q{6.4}$ Run the code below. Which classifier has higher variance?\n",
    "\n",
    "$\\q{6.5}$ When would you prefer a classifier with high bias and low variance? When would you prefer a classifier with low bias and high variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Logistic Regression\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs')]\n",
    "\n",
    "x7, y7 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "x7 += 2 * rng.uniform(size=x7.shape)\n",
    "linearly_separable = (x7, y7)\n",
    "\n",
    "ds = make_moons(noise=0.3, random_state=0)\n",
    "figure = plt.figure(figsize=(15, 25))\n",
    "i = 1\n",
    "n_iterations = 5\n",
    "for iteration in range(n_iterations):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    x7, y7 = ds\n",
    "    x7 = StandardScaler().fit_transform(x7)\n",
    "    x7_train, x7_test, y7_train, y7_test = train_test_split(x7, y7, test_size=.4, random_state=int(time.perf_counter()) + iteration)\n",
    "\n",
    "    x7_min, x7_max = x7[:, 0].min() - .5, x7[:, 0].max() + .5\n",
    "    y7_min, y7_max = x7[:, 1].min() - .5, x7[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x7_min, x7_max, h), np.arange(y7_min, y7_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(n_iterations, len(classifiers) + 1, i)\n",
    "    ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(x7_train[:, 0], x7_train[:, 1], c=y7_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(x7_test[:, 0], x7_test[:, 1], c=y7_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(n_iterations, len(classifiers) + 1, i)\n",
    "        clf.fit(x7_train, y7_train)\n",
    "        score = clf.score(x7_test, y7_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(x7_train[:, 0], x7_train[:, 1], c=y7_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(x7_test[:, 0], x7_test[:, 1], c=y7_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
